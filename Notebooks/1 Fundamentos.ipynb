{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c7207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2a319b",
   "metadata": {},
   "source": [
    "# WebScrapping using BeautifulSoup\n",
    "BeautifulSoup e Requests são duas bibliotecas comuns em Python para web scraping. Com a biblioteca Requests, você faz uma requisição HTTP para obter o HTML da página a partir de um URL. Com o HTML em mãos, você usa o BeautifulSoup para navegar pela tags e classes do documento e extrai as informações de interesse.\n",
    "\n",
    "O presente notebook tem por objetivo fazer desenvolver algumas funções e utilizades simples quanto as bibliotecas e por sua vez demonstrar algumas possiiblidades de ações."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b818e1d6",
   "metadata": {},
   "source": [
    "# Declaração de funções com uso do BS4\n",
    "A partir das funções do beautifulsoup(BS4) desenvolvi algumas funções simples que podem auxiliar na busca de textos, links e notícias em uma pesquisa do Google. A ideia é tornar a busca mais dinâmica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060019c5",
   "metadata": {},
   "source": [
    "### busca_google()\n",
    "Para tal, observei que o padrão de adição de palavras na query de busca do Google separa cada palavra da busca na barra por \"+\", sendo assim a primeira função realiza uma busca no Google a partir das palavras recebidas como parâmetros, caso não receba nenhum parâmetros a função faz a requisição dos termos de busca por um input.\n",
    "\n",
    "OBS:  \n",
    "**'User-Agent' no  header do request:** Enviar o user-agent no header faz com que o servidor retorne o mesmo conteúdo que enviaria a um navegador real; sem isso, a resposta pode ser diferente ou bloqueada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_google(termos = None):\n",
    "    soup = None\n",
    "    # Garante que o retorno do request terá o mesmo conteúdo do navegador\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'}\n",
    "\n",
    "    if termos is None:\n",
    "        termos = input(\"Pesquisar no google:\")\n",
    "    else:\n",
    "        termos = termos.replace(\" \", \"+\")\n",
    "    \n",
    "    url = 'https://www.google.com/search?q='+termos\n",
    "    pagina = requests.get(url, headers = headers)\n",
    "\n",
    "    if pagina.status_code == 200:\n",
    "        print(\"Website found successfully :)\")\n",
    "        print(url) ## Exibindo a url para poder acompanhar os links e tags da página também no navegador\n",
    "        html = pagina.text\n",
    "        soup = BeautifulSoup(html, features='html.parser')\n",
    "        return soup\n",
    "    else: \n",
    "        print(\"Something went wrong :(\")\n",
    "        return None\n",
    "    \n",
    "soup = busca_google(\"campo receptivo cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1115dd88",
   "metadata": {},
   "source": [
    "### seleciona_atr()\n",
    "Essa função  recebe como parâmetros um objeto BS4, o seletor HTML e o atributo desejado. Assim, a função percorre elementos HTML do BS4 extraindo os valores selecionados de um atributo específico ou o texto contido neles, com a opção de personalizar o elemento e o atributo desejado.\n",
    "\n",
    "Como a retirada do texto das tags utiliza uma função distinta para que trechos html não venham juntos se o atributo for \"text\", o texto de cada elemento é adicionado à lista de retorno. Caso contrário, a função verifica se o atributo existe e, se sim, adiciona o valor correspondente; se o atributo não existir, None é adicionado à lista. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seleciona_atr(soup, selecao = 'a', atr = 'href'):    \n",
    "    retorno = []\n",
    "    items = soup.select(selecao)\n",
    "    for item in items:\n",
    "        if atr == \"text\":\n",
    "            texto = item.get_text()\n",
    "            retorno.append(item.get_text())\n",
    "\n",
    "        elif item.has_attr(atr): \n",
    "            valor = item.get(atr) # Caso o atributo não existisse o retorno da função seria None\n",
    "            retorno.append(valor)\n",
    "        else:\n",
    "            retorno.append(None)\n",
    "    return retorno\n",
    " \n",
    " # ALGUNS EXEMPLOS DE USO:\n",
    " \n",
    "# links encontrados da busca\n",
    "seleciona_atr(soup,selecao= \"div.yuRUbf>div>span>a\", atr =\"href\")\n",
    "\n",
    "# Links das próximas paginas da busca\n",
    "seleciona_atr(soup,selecao= \"a.fl\", atr =\"href\")\n",
    "\n",
    "# TEXTOS\n",
    "seleciona_atr(soup,selecao= \"div.VwiC3b>span\", atr =\"text\")\n",
    "\n",
    "# TÍTULOS\n",
    "seleciona_atr(soup,selecao= \"h3.LC20lb\", atr =\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc0ff13",
   "metadata": {},
   "source": [
    "## cria_soup()\n",
    "Apesar de parecer redundante, fiz uma função separada para criar objetos BS4 independentes. O objetivo é que a partir da minha busca google, selecionar os links indexados e tentar fazer a busca do conteúdo dos links. Fazendo assim um aprofundamento do conteúdo das buscas nos sites indexados ao google.  \n",
    "\n",
    "Para evitar quebras nos códigos eu inclui um _try-catch_, pois alguns dos links encontrados recorrentemente no google são de arquivos PDF, WORD e não apenas páginas web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cria_soup(url = None):\n",
    "    soup = None\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'}\n",
    "\n",
    "    try:\n",
    "        pagina = requests.get(url, headers=headers)\n",
    "        if pagina.status_code == 200:\n",
    "            html = pagina.text\n",
    "            soup = BeautifulSoup(html, features='html.parser')\n",
    "            return soup\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Exemplo de aplicação:\n",
    "busca_soup = busca_google(\"campo receptivo cnn\")\n",
    "links_indexados = seleciona_atr(busca_soup,selecao= \"div.yuRUbf>div>span>a\", atr =\"href\") # links encontrados da primeira busca\n",
    "soups_links = []\n",
    "for link in links_indexados:\n",
    "    soup_aux = cria_soup(link)\n",
    "    \n",
    "    # Verificar se 'soup' é do tipo BeautifulSoup\n",
    "    if isinstance(soup_aux, BeautifulSoup):\n",
    "        soups_links.append(soup_aux)\n",
    "\n",
    "print(f\"Soups encontrados: {len(soups_links)}\") \n",
    "# Tipos dentro da lista\n",
    "for obj in soups_links:\n",
    "    print(type(obj))\n",
    "# Conteúdo\n",
    "print((soups_links[2].prettify()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c37922",
   "metadata": {},
   "source": [
    "## extrair_texto_limpo()\n",
    "Visto que não estou utilizando uma estrutura única de seleção de texto a partir de agora, o conteúdo da página ao ser retornado pode conter muito mais textos indesejados e que não fazem sentido para uma análise dos texto de conteúdo das páginas. Sendo assim, esta função tem como objetivo remeover elementos da estrutura de páginas de websites que geralmente não contêm um conteúdo relevante do assunto.  \n",
    "A função identifica e remove tags HTML que não são relevantes para a análise de conteúdo, como <\\script>, <\\style>, <\\header>, <\\footer>, <\\nav>, e <\\aside>. Esses elementos são comuns em páginas web, mas normalmente não contêm o texto principal que você deseja extrair.\n",
    "\n",
    "Utilizando soup.get_text(separator=\" \") o conteúdo textual das diferentes partes da página são concatenados com um espaço como separador, criando uma continuidade no texto.\n",
    "\n",
    "Para melhorar ainda mais a legibilidade e a utilidade do texto extraído, a função remove múltiplas quebras de linha e espaços em branco extras. Isso é feito dividindo o texto em uma lista de palavras e, em seguida, juntando essas palavras com um único espaço entre elas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_texto_limpo(soup):\n",
    "    # Remove elementos desnecessários\n",
    "    for script in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\", \"aside\"]):\n",
    "        script.extract()\n",
    "    \n",
    "    # Extrair o texto\n",
    "    texto = soup.get_text(separator=\" \")\n",
    "\n",
    "    # Limpar o texto, removendo múltiplas quebras de linha e espaços extras\n",
    "    texto_limpo = ' '.join(texto.split())\n",
    "    \n",
    "    return texto_limpo\n",
    "\n",
    "\n",
    "print(extrair_texto_limpo(soups_links[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a37d84",
   "metadata": {},
   "source": [
    "## remove_stopwords()\n",
    "Para fazer a análise do conteúdo dos texto de forma sumarizada, sistemática e menos qualitativa farei a remoção das _Stopwords (palavras de pouca relevância)_ do texto utilizando ferramentas da biblioteca `NLTK`. Esta função baixa o conjunto de stopwords  de diversos idiomas, incluindo o português. As stopwords são armazenadas localmente e usadas para filtrar palavras comuns que geralmente não agregam muito significado ao texto.\n",
    "\n",
    "O texto é divido em palavras e então são removidas as stopwords pela função de tokenização da NLTK. Se a palavra não estiver na lista de stopwords, ela é mantida; caso contrário, é removida.\n",
    "\n",
    "Essa função pode ser usada como parte de um pipeline de pré-processamento de texto em projetos de NLP, tornando mais fácil e rápido limpar e preparar dados textuais.\n",
    "\n",
    "OBS:\n",
    "- Um corpus é uma coleção de textos que podem ser usados para análise de linguagem natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84905e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparação do ambiente:\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # É um dos conjuntos de dados inclusos em corpus incluindo diferentes idiomas.\n",
    "from nltk.tokenize import word_tokenize # Contém funções para dividir o texto em unidades menores, como palavras ou frases.\n",
    "\n",
    "#nltk.download('stopwords') # Garante que as listas de stopwords dos idiomas estejam disponíveis localmente no ambiente de desenvolvimento\n",
    "#nltk.download('punkt') # A word_tokenize() para dividir uma string de texto em palavras, o NLTK utiliza o modelo punkt para identificar corretamente os limites das palavras\n",
    "#nltk.download('punkt_tab')\n",
    "def remove_stopwords(texto, idioma = \"portuguese\"):\n",
    "    # Obter a lista de stopwords para o idioma especificado\n",
    "    stop_words = set(stopwords.words(idioma))\n",
    "    # Tokenizar o texto\n",
    "    palavras = word_tokenize(texto)\n",
    "    \n",
    "    # Filtrar as palavras que não são stopwords\n",
    "    palavras_filtradas = []\n",
    "    for palavra in palavras:\n",
    "        if palavra.lower() not in stop_words and palavra.isalpha():\n",
    "            palavras_filtradas.append(palavra.lower())\n",
    "    \n",
    "    # Juntar as palavras filtradas de volta em uma string\n",
    "    texto_limpo = ' '.join(palavras_filtradas)\n",
    "    \n",
    "    return texto_limpo\n",
    "\n",
    "# Exemplo de uso\n",
    "texto_exemplo = \"Este é um exemplo de frase com algumas palavras irrelevantes.\"\n",
    "texto_processado = remove_stopwords(texto_exemplo)\n",
    "print(texto_processado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdc5efb",
   "metadata": {},
   "source": [
    "## verifica_texto_legivel()\n",
    "A função determina se o texto extraído de um documento ou página web é, de fato, \"legível\" e não apenas uma sequência de caracteres sem sentido, o que pode ocorrer caso o contéudo da página exista mas esteja em um formato de mídia ou arquivo binario. A ideia é verificar se a maior parte do texto consiste em caracteres alfanuméricos com letras, números e espaços.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f54bcf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def verificar_texto_legivel(texto):\n",
    "    # Conta o número de caracteres \"legíveis\", ou seja, letras, números e espaços\n",
    "    caracteres_legiveis = sum(c.isalnum() or c.isspace() for c in texto)\n",
    "    \n",
    "    # Calcula a proporção de caracteres legíveis em relação ao comprimento total do texto\n",
    "    proporcao_legivel = caracteres_legiveis / len(texto)\n",
    "    \n",
    "    # Se mais de 70% do texto é legível, consideramos que é texto válido\n",
    "    return proporcao_legivel > 0.7\n",
    "\n",
    "texto_valido = \"Este é um exemplo de texto legível e normal.\"\n",
    "texto_invalido = \"\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x09\"\n",
    "\n",
    "print(verificar_texto_legivel(texto_valido))   # Deve retornar True\n",
    "print(verificar_texto_legivel(texto_invalido)) # Deve retornar False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
