{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e2a319b",
   "metadata": {},
   "source": [
    "# WebScrapping using BeautifulSoup\n",
    "BeautifulSoup e Requests são duas bibliotecas comuns em Python para web scraping. Com a biblioteca Requests, você faz uma requisição HTTP para obter o HTML da página a partir de um URL. Com o HTML em mãos, você usa o BeautifulSoup para navegar pela tags e classes do documento e extrai as informações de interesse.\n",
    "\n",
    "O presente notebook tem por objetivo fazer desenvolver algumas funções e utilizades simples quanto as bibliotecas e por sua vez demonstrar algumas possiiblidades de ações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c7207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b818e1d6",
   "metadata": {},
   "source": [
    "# Declaração de funções com uso do BS4\n",
    "A partir das funções do beautifulsoup(BS4) desenvolvi algumas funções simples que podem auxiliar na busca de textos, links e notícias em uma pesquisa do Google. A ideia é tornar a busca mais dinâmica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060019c5",
   "metadata": {},
   "source": [
    "### busca_google()\n",
    "Para tal, observei que o padrão de adição de palavras na query de busca do Google separa cada palavra da busca na barra por \"+\", sendo assim a primeira função realiza uma busca no Google a partir das palavras recebidas como parâmetros, caso não receba nenhum parâmetros a função faz a requisição dos termos de busca por um input.\n",
    "\n",
    "OBS:  \n",
    "**'User-Agent' no  header do request:** Enviar o user-agent no header faz com que o servidor retorne o mesmo conteúdo que enviaria a um navegador real; sem isso, a resposta pode ser diferente ou bloqueada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_google(termos = None):\n",
    "    soup = None\n",
    "    # Garante que o retorno do request terá o mesmo conteúdo do navegador\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'}\n",
    "\n",
    "    if termos is None:\n",
    "        termos = input(\"Pesquisar no google:\")\n",
    "    else:\n",
    "        termos = termos.replace(\" \", \"+\")\n",
    "    \n",
    "    url = 'https://www.google.com/search?q='+termos\n",
    "    pagina = requests.get(url, headers = headers)\n",
    "\n",
    "    if pagina.status_code == 200:\n",
    "        print(\"Website found successfully :)\")\n",
    "        print(url) ## Exibindo a url para poder acompanhar os links e tags da página também no navegador\n",
    "        html = pagina.text\n",
    "        soup = BeautifulSoup(html, features='html.parser')\n",
    "        return soup\n",
    "    else: \n",
    "        print(\"Something went wrong :(\")\n",
    "        return None\n",
    "    \n",
    "soup = busca_google(\"campo receptivo cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1115dd88",
   "metadata": {},
   "source": [
    "### seleciona_atr()\n",
    "Essa função  recebe como parâmetros um objeto BS4, o seletor HTML e o atributo desejado. Assim, a função percorre elementos HTML do BS4 extraindo os valores selecionados de um atributo específico ou o texto contido neles, com a opção de personalizar o elemento e o atributo desejado.\n",
    "\n",
    "Como a retirada do texto das tags utiliza uma função distinta para que trechos html não venham juntos se o atributo for \"text\", o texto de cada elemento é adicionado à lista de retorno. Caso contrário, a função verifica se o atributo existe e, se sim, adiciona o valor correspondente; se o atributo não existir, None é adicionado à lista. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seleciona_atr(soup, selecao = 'a', atr = 'href'):    \n",
    "    retorno = []\n",
    "    items = soup.select(selecao)\n",
    "    for item in items:\n",
    "        if atr == \"text\":\n",
    "            texto = item.get_text()\n",
    "            retorno.append(item.get_text())\n",
    "\n",
    "        elif item.has_attr(atr): \n",
    "            valor = item.get(atr) # Caso o atributo não existisse o retorno da função seria None\n",
    "            retorno.append(valor)\n",
    "        else:\n",
    "            retorno.append(None)\n",
    "    return retorno\n",
    " \n",
    " # ALGUNS EXEMPLOS DE USO:\n",
    " \n",
    "# links encontrados da busca\n",
    "seleciona_atr(soup,selecao= \"div.yuRUbf>div>span>a\", atr =\"href\")\n",
    "\n",
    "# Links das próximas paginas da busca\n",
    "seleciona_atr(soup,selecao= \"a.fl\", atr =\"href\")\n",
    "\n",
    "# TEXTOS\n",
    "seleciona_atr(soup,selecao= \"div.VwiC3b>span\", atr =\"text\")\n",
    "\n",
    "# TÍTULOS\n",
    "seleciona_atr(soup,selecao= \"h3.LC20lb\", atr =\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc0ff13",
   "metadata": {},
   "source": [
    "## cria_soup()\n",
    "Apesar de parecer redundante, fiz uma função separada para criar objetos BS4 independentes. O objetivo é que a partir da minha busca google, selecionar os links indexados e tentar fazer a busca do conteúdo dos links. Fazendo assim um aprofundamento do conteúdo das buscas nos sites indexados ao google.  \n",
    "\n",
    "Para evitar quebras nos códigos eu inclui um _try-catch_, pois alguns dos links encontrados recorrentemente no google são de arquivos PDF, WORD e não apenas páginas web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cria_soup(url = None):\n",
    "    soup = None\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'}\n",
    "\n",
    "    try:\n",
    "        pagina = requests.get(url, headers=headers)\n",
    "        if pagina.status_code == 200:\n",
    "            html = pagina.text\n",
    "            soup = BeautifulSoup(html, features='html.parser')\n",
    "            return soup\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Exemplo de aplicação:\n",
    "busca_soup = busca_google(\"campo receptivo cnn\")\n",
    "links_indexados = seleciona_atr(busca_soup,selecao= \"div.yuRUbf>div>span>a\", atr =\"href\") # links encontrados da primeira busca\n",
    "soups_links = []\n",
    "for link in links_indexados:\n",
    "    soup_aux = cria_soup(link)\n",
    "    \n",
    "    # Verificar se 'soup' é do tipo BeautifulSoup\n",
    "    if isinstance(soup_aux, BeautifulSoup):\n",
    "        soups_links.append(soup_aux)\n",
    "\n",
    "print(f\"Soups encontrados: {len(soups_links)}\") \n",
    "# Tipos dentro da lista\n",
    "for obj in soups_links:\n",
    "    print(type(obj))\n",
    "# Conteúdo\n",
    "print((soups_links[2].prettify()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c37922",
   "metadata": {},
   "source": [
    "## extrair_texto_limpo()\n",
    "Visto que não estou utilizando uma estrutura única de seleção de texto a partir de agora, o conteúdo da página ao ser retornado pode conter muito mais textos indesejados e que não fazem sentido para uma análise dos texto de conteúdo das páginas. Sendo assim, esta função tem como objetivo remeover elementos da estrutura de páginas de websites que geralmente não contêm um conteúdo relevante do assunto.  \n",
    "A função identifica e remove tags HTML que não são relevantes para a análise de conteúdo, como <script>, <style>, <header>, <footer>, <nav>, e <aside>. Esses elementos são comuns em páginas web, mas normalmente não contêm o texto principal que você deseja extrair.\n",
    "\n",
    "Utilizando soup.get_text(separator=\" \") o conteúdo textual das diferentes partes da página são concatenados com um espaço como separador, criando uma continuidade no texto.\n",
    "\n",
    "Para melhorar ainda mais a legibilidade e a utilidade do texto extraído, a função remove múltiplas quebras de linha e espaços em branco extras. Isso é feito dividindo o texto em uma lista de palavras e, em seguida, juntando essas palavras com um único espaço entre elas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_texto_limpo(soup):\n",
    "    # Remove elementos desnecessários\n",
    "    for script in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\", \"aside\"]):\n",
    "        script.extract()\n",
    "    \n",
    "    # Extrair o texto\n",
    "    texto = soup.get_text(separator=\" \")\n",
    "\n",
    "    # Limpar o texto, removendo múltiplas quebras de linha e espaços extras\n",
    "    texto_limpo = ' '.join(texto.split())\n",
    "    \n",
    "    return texto_limpo\n",
    "\n",
    "\n",
    "print(extrair_texto_limpo(soups_links[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a37d84",
   "metadata": {},
   "source": [
    "## remove_stopwords()\n",
    "Para fazer a análise do conteúdo dos texto de forma sumarizada, sistemática e menos qualitativa farei a remoção das _Stopwords (palavras de pouca relevância)_ do texto utilizando ferramentas da biblioteca `NLTK`. Esta função baixa o conjunto de stopwords  de diversos idiomas, incluindo o português. As stopwords são armazenadas localmente e usadas para filtrar palavras comuns que geralmente não agregam muito significado ao texto.\n",
    "\n",
    "O texto é divido em palavras e então são removidas as stopwords pela função de tokenização da NLTK. Se a palavra não estiver na lista de stopwords, ela é mantida; caso contrário, é removida.\n",
    "\n",
    "Essa função pode ser usada como parte de um pipeline de pré-processamento de texto em projetos de NLP, tornando mais fácil e rápido limpar e preparar dados textuais.\n",
    "\n",
    "OBS:\n",
    "- Um corpus é uma coleção de textos que podem ser usados para análise de linguagem natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84905e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparação do ambiente:\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # É um dos conjuntos de dados inclusos em corpus incluindo diferentes idiomas.\n",
    "from nltk.tokenize import word_tokenize # Contém funções para dividir o texto em unidades menores, como palavras ou frases.\n",
    "\n",
    "#nltk.download('stopwords') # Garante que as listas de stopwords dos idiomas estejam disponíveis localmente no ambiente de desenvolvimento\n",
    "#nltk.download('punkt') # A word_tokenize() para dividir uma string de texto em palavras, o NLTK utiliza o modelo punkt para identificar corretamente os limites das palavras\n",
    "#nltk.download('punkt_tab')\n",
    "def remove_stopwords(texto, idioma = \"portuguese\"):\n",
    "    # Obter a lista de stopwords para o idioma especificado\n",
    "    stop_words = set(stopwords.words(idioma))\n",
    "    # Tokenizar o texto\n",
    "    palavras = word_tokenize(texto)\n",
    "    \n",
    "    # Filtrar as palavras que não são stopwords\n",
    "    palavras_filtradas = []\n",
    "    for palavra in palavras:\n",
    "        if palavra.lower() not in stop_words:\n",
    "            palavras_filtradas.append(palavra)\n",
    "    \n",
    "    # Juntar as palavras filtradas de volta em uma string\n",
    "    texto_limpo = ' '.join(palavras_filtradas)\n",
    "    \n",
    "    return texto_limpo\n",
    "\n",
    "# Exemplo de uso\n",
    "texto_exemplo = \"Este é um exemplo de frase com algumas palavras irrelevantes.\"\n",
    "texto_processado = remove_stopwords(texto_exemplo)\n",
    "print(texto_processado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149def3d",
   "metadata": {},
   "source": [
    "## CASE 1: Principais palavras de um assunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8347aadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website found successfully :)\n",
      "https://www.google.com/search?q=campo+receptivo+cnn\n",
      "An error occurred: HTTPSConnectionPool(host='ww2.inf.ufg.br', port=443): Max retries exceeded with url: /~anderson/deeplearning/Deep%20Learning%20-%20Redes%20Neurais%20Profundas%20Convolucionais.pdf (Caused by SSLError(SSLError(1, '[SSL: DH_KEY_TOO_SMALL] dh key too small (_ssl.c:1007)')))\n",
      "\n",
      "Soups encontrados: 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import func_webscrapping\n",
    "from func_webscrapping import busca_google, seleciona_atr, cria_soup, extrair_texto_limpo, remove_stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Exemplo de aplicação:\n",
    "busca_soup = busca_google(\"campo receptivo cnn\")\n",
    "\n",
    "links_indexados = seleciona_atr(busca_soup,selecao= \"div.yuRUbf>div>span>a\", atr =\"href\") # links encontrados na primeira página da busca\n",
    "\n",
    "soups_links = [] # Lista de objetos soups originados a partir dos links indexados na busca\n",
    "for link in links_indexados:\n",
    "    soup_aux = cria_soup(link)\n",
    "    # Verifica se 'soup_aux' é do tipo BeautifulSoup e se sim adiciona ele na lista \n",
    "    if isinstance(soup_aux, BeautifulSoup):\n",
    "        soups_links.append(soup_aux)\n",
    "\n",
    "# Exibe o número de páginas capazes de serem análisadas\n",
    "print(f\"\\nSoups encontrados: {len(soups_links)}\\n\") \n",
    "\n",
    "conteudos = [] # Lista contendo strings com o conteúdo texto das páginas\n",
    "for soup_link in soups_links:\n",
    "    # Extrai o texto do que tem maior chance de ser conteúdo das páginas\n",
    "    texto = extrair_texto_limpo(soup_link) \n",
    "    # Aplica funções do NLTK para remover stopword em português\n",
    "    texto_processado = remove_stopwords(texto)\n",
    "\n",
    "    if len(texto_processado) > 300: # Define um número mínimo de caracters para que o conteúdo do site possa ser definido como útil a pesquisa\n",
    "        conteudos.append(texto_processado)\n",
    "\n",
    "print(conteudos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "tokenized_texts\n",
    "flat_tokens = [item for sublist in tokenized_texts for item in sublist]\n",
    "\n",
    "# Contar a frequência das palavras\n",
    "word_freq = Counter(flat_tokens)\n",
    "\n",
    "# Cálculo do TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_scores = X.mean(axis=0).A1\n",
    "word_tfidf = dict(zip(feature_names, tfidf_scores))\n",
    "\n",
    "# Criação de DataFrame para análise\n",
    "df = pd.DataFrame.from_dict(word_freq, orient='index', columns=['Frequency'])\n",
    "df['IDF'] = df.index.map(word_tfidf)\n",
    "df = df.dropna()  # Remove palavras sem IDF\n",
    "\n",
    "# Indexação de cada palavra com IDF e Frequência\n",
    "print(\"Indexação das palavras com IDF e Frequência:\")\n",
    "print(df)\n",
    "\n",
    "# Filtra as 10 palavras mais frequentes\n",
    "top_10_words = df.nlargest(10, 'Frequency')\n",
    "\n",
    "# Scatterplot: Frequências em x e IDF em tamanho do dot\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(top_10_words.index, top_10_words['Frequency'], s=top_10_words['IDF']*1000, alpha=0.6)\n",
    "plt.xlabel('Palavras')\n",
    "plt.ylabel('Frequência')\n",
    "plt.title('Palavras vs Frequência com IDF como Tamanho do Dot')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Scatterplot: IDF em x e Frequências em tamanho do dot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(top_10_words['IDF'], top_10_words['Frequency'], s=top_10_words['IDF']*1000, alpha=0.6)\n",
    "plt.xlabel('IDF')\n",
    "plt.ylabel('Frequência')\n",
    "plt.title('IDF vs Frequência com Frequência como Tamanho do Dot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
